{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connected to dev-chartter-v2-py3.13 (Python 3.13.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edc0f35-041e-4ca8-be7c-79eabd0f9ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_analyzer(df: pd.DataFrame,\n",
    "                   config: Optional[TechnicalAnalysisConfig] = None,\n",
    "                   validate_ohlcv: bool = True) -> EnhancedTechnicalAnalyzer:\n",
    "    \"\"\"Factory function to create EnhancedTechnicalAnalyzer instance\"\"\"\n",
    "    return EnhancedTechnicalAnalyzer(df, config, validate_ohlcv)\n",
    "\n",
    "# Alias for backward compatibility\n",
    "def cabr(df: pd.DataFrame) -> EnhancedTechnicalAnalyzer:\n",
    "    \"\"\"Legacy factory function for compatibility\"\"\"\n",
    "    return EnhancedTechnicalAnalyzer(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6702fe78-9528-4917-a8ba-0e0cce7f1dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Enhanced Enterprise Technical Analysis Framework - Refactored\n",
    "Clean architecture with separated concerns, proper caching, and optimized performance\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import talib\n",
    "from typing import Union, Optional, Dict, List, Callable, Any, Tuple, Protocol\n",
    "from abc import ABC, abstractmethod\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "import re\n",
    "from functools import lru_cache, wraps\n",
    "import warnings\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import multiprocessing as mp\n",
    "from contextlib import contextmanager\n",
    "import hashlib\n",
    "import time\n",
    "import tracemalloc\n",
    "import gc\n",
    "\n",
    "# Third-party imports for enhanced caching\n",
    "try:\n",
    "    from cachetools import TTLCache, LRUCache\n",
    "except ImportError:\n",
    "    # Fallback to basic dict if cachetools not available\n",
    "    TTLCache = dict\n",
    "    LRUCache = dict\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# %%\n",
    "@dataclass(frozen=True)\n",
    "class TechnicalAnalysisConfig:\n",
    "    \"\"\"Configuration constants for the framework\"\"\"\n",
    "    # Data validation\n",
    "    MAX_NAN_RATIO: float = 0.5\n",
    "    MIN_DATA_POINTS: int = 50\n",
    "\n",
    "    # Caching\n",
    "    CACHE_SIZE: int = 1000\n",
    "    CACHE_TTL: int = 3600  # 1 hour\n",
    "\n",
    "    # Performance\n",
    "    MAX_WORKERS: int = 4\n",
    "    CHUNK_SIZE: int = 10000\n",
    "\n",
    "    # Bollinger Bands\n",
    "    DEFAULT_BB_PERIOD: int = 20\n",
    "    DEFAULT_BB_STD: float = 2.0\n",
    "\n",
    "    # Memory optimization\n",
    "    OPTIMIZE_MEMORY: bool = True\n",
    "    KEEP_OHLCV_FLOAT64: bool = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36317c3-55a6-4a98-856e-73ec049b719c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComparisonType(Enum):\n",
    "    \"\"\"Enumeration of supported comparison operations\"\"\"\n",
    "    ABOVE = \"above\"\n",
    "    BELOW = \"below\"\n",
    "    CROSSED_UP = \"crossed_up\"\n",
    "    CROSSED_DOWN = \"crossed_down\"\n",
    "    EQUALS = \"equals\"\n",
    "    GREATER_EQUAL = \"greater_equal\"\n",
    "    LESS_EQUAL = \"less_equal\"\n",
    "\n",
    "class IndicatorType(Enum):\n",
    "    \"\"\"Enumeration of TALib indicator categories\"\"\"\n",
    "    OVERLAP = \"overlap\"\n",
    "    MOMENTUM = \"momentum\"\n",
    "    VOLUME = \"volume\"\n",
    "    VOLATILITY = \"volatility\"\n",
    "    PRICE = \"price\"\n",
    "    CYCLE = \"cycle\"\n",
    "    PATTERN = \"pattern\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88c15c5-bba5-45fe-ae94-d603c3a41b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AnalysisResult:\n",
    "    \"\"\"Enhanced data class to encapsulate analysis results\"\"\"\n",
    "    column_name: str\n",
    "    operation: str\n",
    "    success: bool\n",
    "    message: str = \"\"\n",
    "    data: Optional[pd.Series] = None\n",
    "    execution_time: float = 0.0\n",
    "    memory_usage: int = 0\n",
    "\n",
    "@dataclass(frozen=True)  # Make it hashable for caching\n",
    "class IndicatorConfig:\n",
    "    \"\"\"Immutable configuration for technical indicators\"\"\"\n",
    "    name: str\n",
    "    period: Optional[int] = None\n",
    "    fast_period: Optional[int] = None\n",
    "    slow_period: Optional[int] = None\n",
    "    signal_period: Optional[int] = None\n",
    "    source_column: str = \"Close\"\n",
    "    parameters: Tuple[Tuple[str, Any], ...] = field(default_factory=tuple)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Convert dict to tuple of tuples for hashability\n",
    "        if hasattr(self.parameters, 'items'):\n",
    "            object.__setattr__(self, 'parameters', tuple(self.parameters.items()))\n",
    "\n",
    "class TAException(Exception):\n",
    "    \"\"\"Enhanced exception for Technical Analysis operations\"\"\"\n",
    "    def __init__(self, message: str, error_code: str = \"GENERAL\", details: Optional[Dict] = None):\n",
    "        self.error_code = error_code\n",
    "        self.details = details or {}\n",
    "        super().__init__(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fab57a5-9f07-4bce-ba8b-5ade4a10748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceProfiler:\n",
    "    \"\"\"Performance monitoring and optimization utilities\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def profile_execution(func: Callable) -> Callable:\n",
    "        \"\"\"Decorator to profile function execution time and memory\"\"\"\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            tracemalloc.start()\n",
    "            start_time = time.perf_counter()\n",
    "\n",
    "            try:\n",
    "                result = func(*args, **kwargs)\n",
    "                return result\n",
    "            finally:\n",
    "                execution_time = time.perf_counter() - start_time\n",
    "                current, peak = tracemalloc.get_traced_memory()\n",
    "                tracemalloc.stop()\n",
    "\n",
    "                logger.debug(f\"{func.__name__} executed in {execution_time:.4f}s, \"\n",
    "                           f\"memory: {current / 1024 / 1024:.2f}MB\")\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    @staticmethod\n",
    "    @contextmanager\n",
    "    def memory_efficient_processing():\n",
    "        \"\"\"Context manager for memory-efficient processing\"\"\"\n",
    "        gc.collect()\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a498180c-ba2d-438a-af39-b20526bb7fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CacheManager:\n",
    "    \"\"\"Centralized cache management with configurable strategies\"\"\"\n",
    "\n",
    "    def __init__(self, config: TechnicalAnalysisConfig):\n",
    "        self.config = config\n",
    "        if TTLCache != dict:\n",
    "            self._cache = TTLCache(maxsize=config.CACHE_SIZE, ttl=config.CACHE_TTL)\n",
    "        else:\n",
    "            self._cache = {}\n",
    "            self._max_size = config.CACHE_SIZE\n",
    "\n",
    "    def get(self, key: str) -> Any:\n",
    "        \"\"\"Get item from cache\"\"\"\n",
    "        return self._cache.get(key)\n",
    "\n",
    "    def put(self, key: str, value: Any) -> None:\n",
    "        \"\"\"Put item in cache with size management\"\"\"\n",
    "        if TTLCache == dict and len(self._cache) >= self._max_size:\n",
    "            # Simple LRU eviction\n",
    "            oldest_key = next(iter(self._cache))\n",
    "            del self._cache[oldest_key]\n",
    "\n",
    "        self._cache[key] = value\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear cache\"\"\"\n",
    "        self._cache.clear()\n",
    "\n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get cache statistics\"\"\"\n",
    "        return {\n",
    "            'size': len(self._cache),\n",
    "            'max_size': getattr(self, '_max_size', self.config.CACHE_SIZE),\n",
    "            'hit_rate': getattr(self._cache, 'hits', 0) / max(getattr(self._cache, 'misses', 1) + getattr(self._cache, 'hits', 0), 1)\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def create_efficient_key(*args) -> str:\n",
    "        \"\"\"Create efficient cache key using hashing\"\"\"\n",
    "        key_data = str(args).encode('utf-8')\n",
    "        return hashlib.md5(key_data).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163798d6-cf3b-4443-a45a-9217e72b00d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataValidator:\n",
    "    \"\"\"Enhanced data validation with optimized performance\"\"\"\n",
    "\n",
    "    def __init__(self, config: TechnicalAnalysisConfig):\n",
    "        self.config = config\n",
    "\n",
    "    @lru_cache(maxsize=128)\n",
    "    def validate_column_exists(self, columns_hash: int, column: str) -> bool:\n",
    "        \"\"\"Cached column existence validation\"\"\"\n",
    "        # Note: This requires pre-computed hash of columns\n",
    "        return True  # Actual validation done in calling code\n",
    "\n",
    "    def validate_numeric_column(self, df: pd.DataFrame, column: str) -> bool:\n",
    "        \"\"\"Enhanced numeric column validation\"\"\"\n",
    "        if column not in df.columns:\n",
    "            available = list(df.columns)\n",
    "            raise TAException(\n",
    "                f\"Column '{column}' not found\",\n",
    "                \"COLUMN_NOT_FOUND\",\n",
    "                {\"available_columns\": available}\n",
    "            )\n",
    "\n",
    "        if not pd.api.types.is_numeric_dtype(df[column]):\n",
    "            raise TAException(\n",
    "                f\"Column '{column}' must be numeric, got {df[column].dtype}\",\n",
    "                \"INVALID_DTYPE\",\n",
    "                {\"column_dtype\": str(df[column].dtype)}\n",
    "            )\n",
    "\n",
    "        # Check for excessive NaN values\n",
    "        nan_ratio = df[column].isna().sum() / len(df)\n",
    "        if nan_ratio > self.config.MAX_NAN_RATIO:\n",
    "            logger.warning(f\"Column '{column}' has {nan_ratio:.1%} NaN values\")\n",
    "\n",
    "        return True\n",
    "\n",
    "    def validate_ohlcv_data(self, df: pd.DataFrame) -> Dict[str, bool]:\n",
    "        \"\"\"Validate OHLCV data structure\"\"\"\n",
    "        required_cols = ['Open', 'High', 'Low', 'Close']\n",
    "        optional_cols = ['Volume']\n",
    "\n",
    "        validation_results = {}\n",
    "\n",
    "        for col in required_cols:\n",
    "            try:\n",
    "                self.validate_numeric_column(df, col)\n",
    "                validation_results[col] = True\n",
    "            except TAException:\n",
    "                validation_results[col] = False\n",
    "                logger.warning(f\"Required OHLCV column '{col}' is invalid or missing\")\n",
    "\n",
    "        for col in optional_cols:\n",
    "            if col in df.columns:\n",
    "                try:\n",
    "                    self.validate_numeric_column(df, col)\n",
    "                    validation_results[col] = True\n",
    "                except TAException:\n",
    "                    validation_results[col] = False\n",
    "\n",
    "        return validation_results\n",
    "\n",
    "    def validate_minimum_data_points(self, df: pd.DataFrame) -> bool:\n",
    "        \"\"\"Validate minimum data points for analysis\"\"\"\n",
    "        if len(df) < self.config.MIN_DATA_POINTS:\n",
    "            raise TAException(\n",
    "                f\"Insufficient data points: {len(df)} < {self.config.MIN_DATA_POINTS}\",\n",
    "                \"INSUFFICIENT_DATA\"\n",
    "            )\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606a3d2e-f0dd-4974-ad14-cebdbfcacdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataManager:\n",
    "    \"\"\"Separated data management responsibilities\"\"\"\n",
    "\n",
    "    def __init__(self, config: TechnicalAnalysisConfig):\n",
    "        self.config = config\n",
    "        self.validator = DataValidator(config)\n",
    "\n",
    "    def prepare_dataframe(self, df: pd.DataFrame, validate_ohlcv: bool = True) -> pd.DataFrame:\n",
    "        \"\"\"Prepare and optimize DataFrame\"\"\"\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise TAException(\"Input must be a pandas DataFrame\", \"INVALID_INPUT\")\n",
    "\n",
    "        if df.empty:\n",
    "            raise TAException(\"DataFrame cannot be empty\", \"EMPTY_DATAFRAME\")\n",
    "\n",
    "        # Validate minimum data points\n",
    "        self.validator.validate_minimum_data_points(df)\n",
    "\n",
    "        # Validate OHLCV structure\n",
    "        if validate_ohlcv:\n",
    "            validation_results = self.validator.validate_ohlcv_data(df)\n",
    "            logger.info(f\"OHLCV validation: {validation_results}\")\n",
    "\n",
    "        # Create optimized copy\n",
    "        optimized_df = self._optimize_datatypes(df.copy())\n",
    "\n",
    "        return optimized_df\n",
    "\n",
    "    def _optimize_datatypes(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Optimize DataFrame data types with OHLCV preservation\"\"\"\n",
    "        if not self.config.OPTIMIZE_MEMORY:\n",
    "            return df\n",
    "\n",
    "        # Preserve OHLCV columns as float64 for TALib compatibility\n",
    "        protected_cols = ['Open', 'High', 'Low', 'Close', 'Volume'] if self.config.KEEP_OHLCV_FLOAT64 else []\n",
    "\n",
    "        for col in df.select_dtypes(include=['float64']).columns:\n",
    "            if col in protected_cols:\n",
    "                continue\n",
    "\n",
    "            if df[col].dtype == 'float64':\n",
    "                # Check if values can fit in float32\n",
    "                col_min, col_max = df[col].min(), df[col].max()\n",
    "                if (col_min >= np.finfo(np.float32).min and\n",
    "                    col_max <= np.finfo(np.float32).max):\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "\n",
    "        # Optimize integer columns\n",
    "        for col in df.select_dtypes(include=['int64']).columns:\n",
    "            col_min, col_max = df[col].min(), df[col].max()\n",
    "            if col_min >= 0 and col_max <= 255:\n",
    "                df[col] = df[col].astype('uint8')\n",
    "            elif col_min >= -128 and col_max <= 127:\n",
    "                df[col] = df[col].astype('int8')\n",
    "            elif col_min >= -32768 and col_max <= 32767:\n",
    "                df[col] = df[col].astype('int16')\n",
    "            elif col_min >= -2147483648 and col_max <= 2147483647:\n",
    "                df[col] = df[col].astype('int32')\n",
    "\n",
    "        return df\n",
    "\n",
    "    def get_memory_usage(self, df: pd.DataFrame) -> Dict[str, float]:\n",
    "        \"\"\"Get detailed memory usage statistics\"\"\"\n",
    "        memory_usage = df.memory_usage(deep=True)\n",
    "        return {\n",
    "            'total_mb': memory_usage.sum() / 1024 / 1024,\n",
    "            'per_column_mb': {col: usage / 1024 / 1024 for col, usage in memory_usage.items()},\n",
    "            'shape': df.shape\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af9b381-e7bf-455d-8845-a36033ac900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TALibIndicatorEngine:\n",
    "    \"\"\"Optimized TALib indicator calculation engine\"\"\"\n",
    "\n",
    "    def __init__(self, config: TechnicalAnalysisConfig, cache_manager: CacheManager):\n",
    "        self.config = config\n",
    "        self.cache = cache_manager\n",
    "        self._available_indicators = self._get_available_indicators()\n",
    "\n",
    "    @staticmethod\n",
    "    @lru_cache(maxsize=1)\n",
    "    def _get_available_indicators() -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"Cache available TALib indicators with metadata\"\"\"\n",
    "        indicators = {}\n",
    "\n",
    "        for func_name in dir(talib):\n",
    "            if func_name.isupper() and hasattr(talib, func_name):\n",
    "                func = getattr(talib, func_name)\n",
    "                if callable(func):\n",
    "                    try:\n",
    "                        info = talib.abstract.Function(func_name).info\n",
    "                        indicators[func_name] = {\n",
    "                            'function': func,\n",
    "                            'info': info,\n",
    "                            'inputs': info.get('input_names', ['close']),\n",
    "                            'parameters': info.get('parameters', {}),\n",
    "                            'outputs': info.get('output_names', [func_name.lower()])\n",
    "                        }\n",
    "                    except:\n",
    "                        # Fallback for indicators without abstract info\n",
    "                        indicators[func_name] = {\n",
    "                            'function': func,\n",
    "                            'info': {},\n",
    "                            'inputs': ['close'],\n",
    "                            'parameters': {},\n",
    "                            'outputs': [func_name.lower()]\n",
    "                        }\n",
    "\n",
    "        logger.info(f\"Loaded {len(indicators)} TALib indicators\")\n",
    "        return indicators\n",
    "\n",
    "    def is_indicator_available(self, indicator: str) -> bool:\n",
    "        \"\"\"Check if indicator is available\"\"\"\n",
    "        return indicator.upper() in self._available_indicators\n",
    "\n",
    "    def get_indicator_info(self, indicator: str) -> Dict[str, Any]:\n",
    "        \"\"\"Get detailed indicator information\"\"\"\n",
    "        return self._available_indicators.get(indicator.upper(), {})\n",
    "\n",
    "    @PerformanceProfiler.profile_execution\n",
    "    def calculate_indicator(self, df: pd.DataFrame, config: IndicatorConfig) -> pd.Series:\n",
    "        \"\"\"Calculate technical indicator with optimized caching\"\"\"\n",
    "        indicator_name = config.name.upper()\n",
    "\n",
    "        if not self.is_indicator_available(indicator_name):\n",
    "            available = list(self._available_indicators.keys())[:10]  # Show first 10\n",
    "            raise TAException(\n",
    "                f\"Indicator '{indicator_name}' not available in TALib\",\n",
    "                \"INDICATOR_NOT_FOUND\",\n",
    "                {\"available_sample\": available}\n",
    "            )\n",
    "\n",
    "        # Create efficient cache key\n",
    "        cache_key = self._create_cache_key(df, config)\n",
    "\n",
    "        cached_result = self.cache.get(cache_key)\n",
    "        if cached_result is not None:\n",
    "            logger.debug(f\"Cache hit for {indicator_name}\")\n",
    "            return cached_result\n",
    "\n",
    "        try:\n",
    "            result = self._calculate_indicator_internal(df, config)\n",
    "\n",
    "            # Cache the result\n",
    "            self.cache.put(cache_key, result)\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"TALib calculation error for {indicator_name}: {str(e)}\")\n",
    "            raise TAException(\n",
    "                f\"Failed to calculate {indicator_name}: {str(e)}\",\n",
    "                \"CALCULATION_ERROR\",\n",
    "                {\"indicator\": indicator_name, \"config\": config}\n",
    "            )\n",
    "\n",
    "    def _create_cache_key(self, df: pd.DataFrame, config: IndicatorConfig) -> str:\n",
    "        \"\"\"Create efficient cache key using data fingerprint\"\"\"\n",
    "        source_col = df[config.source_column]\n",
    "\n",
    "        # Create data fingerprint instead of hashing entire series\n",
    "        data_fingerprint = (\n",
    "            len(df),\n",
    "            source_col.dtype,\n",
    "            float(source_col.iloc[0]) if len(source_col) > 0 else 0,\n",
    "            float(source_col.iloc[-1]) if len(source_col) > 0 else 0,\n",
    "            float(source_col.mean()) if len(source_col) > 0 else 0\n",
    "        )\n",
    "\n",
    "        # Create config fingerprint\n",
    "        config_data = (\n",
    "            config.name, config.period, config.fast_period,\n",
    "            config.slow_period, config.signal_period, config.source_column\n",
    "        )\n",
    "\n",
    "        return CacheManager.create_efficient_key(data_fingerprint, config_data)\n",
    "\n",
    "    def _calculate_indicator_internal(self, df: pd.DataFrame, config: IndicatorConfig) -> pd.Series:\n",
    "        \"\"\"Internal indicator calculation with proper error handling\"\"\"\n",
    "        indicator_name = config.name.upper()\n",
    "        func_info = self._available_indicators[indicator_name]\n",
    "        func = func_info['function']\n",
    "\n",
    "        # Prepare parameters\n",
    "        kwargs = self._prepare_parameters(config, func_info)\n",
    "\n",
    "        # Get input data with proper types\n",
    "        input_data = self._prepare_input_data(df, config, func_info)\n",
    "\n",
    "        # Special handling for multi-input indicators\n",
    "        result = self._execute_talib_function(func, input_data, kwargs, indicator_name)\n",
    "\n",
    "        # Handle multiple output indicators\n",
    "        if isinstance(result, tuple):\n",
    "            result = self._handle_multiple_outputs(result, indicator_name)\n",
    "\n",
    "        return pd.Series(result, index=df.index, name=f\"{config.name}_{config.period}\" if config.period else config.name)\n",
    "\n",
    "    def _prepare_parameters(self, config: IndicatorConfig, func_info: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Prepare parameters for TALib function\"\"\"\n",
    "        kwargs = {}\n",
    "\n",
    "        # Map standard parameters\n",
    "        if config.period is not None:\n",
    "            kwargs['timeperiod'] = config.period\n",
    "        if config.fast_period is not None:\n",
    "            kwargs['fastperiod'] = config.fast_period\n",
    "        if config.slow_period is not None:\n",
    "            kwargs['slowperiod'] = config.slow_period\n",
    "        if config.signal_period is not None:\n",
    "            kwargs['signalperiod'] = config.signal_period\n",
    "\n",
    "        # Add custom parameters\n",
    "        if config.parameters:\n",
    "            kwargs.update(dict(config.parameters))\n",
    "\n",
    "        return kwargs\n",
    "\n",
    "    def _prepare_input_data(self, df: pd.DataFrame, config: IndicatorConfig, func_info: Dict[str, Any]) -> List[np.ndarray]:\n",
    "        \"\"\"Prepare input data ensuring float64 for TALib compatibility\"\"\"\n",
    "        inputs = func_info.get('inputs', ['close'])\n",
    "        input_data = []\n",
    "\n",
    "        column_mapping = {\n",
    "            'close': config.source_column,\n",
    "            'real': config.source_column,\n",
    "            'high': 'High',\n",
    "            'low': 'Low',\n",
    "            'open': 'Open',\n",
    "            'volume': 'Volume'\n",
    "        }\n",
    "\n",
    "        for inp in inputs:\n",
    "            inp_lower = inp.lower()\n",
    "            target_column = column_mapping.get(inp_lower, config.source_column)\n",
    "\n",
    "            if target_column in df.columns:\n",
    "                data = df[target_column].astype(np.float64).values\n",
    "            else:\n",
    "                # Fallback to source column or default values\n",
    "                if inp_lower == 'volume':\n",
    "                    data = np.ones(len(df), dtype=np.float64)\n",
    "                else:\n",
    "                    data = df[config.source_column].astype(np.float64).values\n",
    "\n",
    "            input_data.append(data)\n",
    "\n",
    "        return input_data\n",
    "\n",
    "    def _execute_talib_function(self, func: Callable, input_data: List[np.ndarray],\n",
    "                               kwargs: Dict[str, Any], indicator_name: str) -> Union[np.ndarray, Tuple]:\n",
    "        \"\"\"Execute TALib function with appropriate input handling\"\"\"\n",
    "        with PerformanceProfiler.memory_efficient_processing():\n",
    "            if indicator_name in ['ADX', 'ADXR', 'AROON', 'AROONOSC', 'CCI', 'DX', 'MFI', 'MINUS_DI', 'MINUS_DM', 'PLUS_DI', 'PLUS_DM', 'RSI', 'STOCH', 'STOCHF', 'STOCHRSI', 'TRIX', 'ULTOSC', 'WILLR']:\n",
    "                # These indicators require High, Low, Close (and sometimes Volume)\n",
    "                if len(input_data) >= 3:\n",
    "                    return func(input_data[0], input_data[1], input_data[2], **kwargs)\n",
    "                else:\n",
    "                    # Fallback: use close price for all inputs\n",
    "                    close_data = input_data[0]\n",
    "                    return func(close_data, close_data, close_data, **kwargs)\n",
    "            else:\n",
    "                # Standard execution based on input count\n",
    "                if len(input_data) == 1:\n",
    "                    return func(input_data[0], **kwargs)\n",
    "                elif len(input_data) == 2:\n",
    "                    return func(input_data[0], input_data[1], **kwargs)\n",
    "                elif len(input_data) == 3:\n",
    "                    return func(input_data[0], input_data[1], input_data[2], **kwargs)\n",
    "                else:\n",
    "                    return func(*input_data, **kwargs)\n",
    "\n",
    "    def _handle_multiple_outputs(self, result: Tuple, indicator_name: str) -> np.ndarray:\n",
    "        \"\"\"Handle indicators that return multiple values\"\"\"\n",
    "        if indicator_name == 'MACD' and len(result) >= 1:\n",
    "            return result[0]  # Return MACD line\n",
    "        elif indicator_name == 'BBANDS' and len(result) >= 3:\n",
    "            return result[1]  # Return middle band (SMA)\n",
    "        elif indicator_name == 'STOCH' and len(result) >= 2:\n",
    "            return result[0]  # Return %K\n",
    "        elif indicator_name == 'AROON' and len(result) >= 2:\n",
    "            return result[0]  # Return Aroon Up\n",
    "        else:\n",
    "            return result[0]  # Default to first output\n",
    "\n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear indicator calculation cache\"\"\"\n",
    "        self.cache.clear()\n",
    "        logger.info(\"Indicator engine cache cleared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e28235a-eac3-4f6c-93ed-656f7bcc6db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseComparator(ABC):\n",
    "    \"\"\"Enhanced abstract base class for comparison operations\"\"\"\n",
    "\n",
    "    def __init__(self, config: TechnicalAnalysisConfig):\n",
    "        self.config = config\n",
    "\n",
    "    @abstractmethod\n",
    "    def compare(self, df: pd.DataFrame, x: str, y: Union[str, float],\n",
    "                new_col: Optional[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"Perform the comparison operation\"\"\"\n",
    "        pass\n",
    "\n",
    "    def _generate_column_name(self, x: str, y: Union[str, float], operation: str) -> str:\n",
    "        \"\"\"Generate descriptive column name with length limits\"\"\"\n",
    "        y_str = str(y).replace('.', '_').replace('-', 'neg')\n",
    "        column_name = f\"{x}_{operation}_{y_str}\"\n",
    "\n",
    "        # Limit column name length\n",
    "        if len(column_name) > 50:\n",
    "            column_name = column_name[:47] + \"...\"\n",
    "\n",
    "        return column_name\n",
    "\n",
    "    def _validate_inputs(self, df: pd.DataFrame, x: str, y: Union[str, float]):\n",
    "        \"\"\"Validate inputs before comparison\"\"\"\n",
    "        if x not in df.columns:\n",
    "            raise TAException(f\"Column '{x}' not found\", \"COLUMN_NOT_FOUND\")\n",
    "\n",
    "        if not pd.api.types.is_numeric_dtype(df[x]):\n",
    "            raise TAException(f\"Column '{x}' must be numeric\", \"INVALID_DTYPE\")\n",
    "\n",
    "        if isinstance(y, str) and y not in df.columns:\n",
    "            raise TAException(f\"Column '{y}' not found\", \"COLUMN_NOT_FOUND\")\n",
    "\n",
    "        if isinstance(y, str) and not pd.api.types.is_numeric_dtype(df[y]):\n",
    "            raise TAException(f\"Column '{y}' must be numeric\", \"INVALID_DTYPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6740b386-13ac-4787-92c6-d7c96d04226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AboveComparator(BaseComparator):\n",
    "    \"\"\"Optimized above comparison with vectorized operations\"\"\"\n",
    "\n",
    "    @PerformanceProfiler.profile_execution\n",
    "    def compare(self, df: pd.DataFrame, x: str, y: Union[str, float],\n",
    "                new_col: Optional[str] = None) -> pd.DataFrame:\n",
    "\n",
    "        self._validate_inputs(df, x, y)\n",
    "\n",
    "        if isinstance(y, (int, float)):\n",
    "            comparison_array = df[x].values > y\n",
    "        else:\n",
    "            comparison_array = df[x].values > df[y].values\n",
    "\n",
    "        new_col = new_col or self._generate_column_name(x, y, \"above\")\n",
    "        df[new_col] = comparison_array.astype(np.int8)  # Use int8 for memory efficiency\n",
    "\n",
    "        return df\n",
    "\n",
    "class BelowComparator(BaseComparator):\n",
    "    \"\"\"Optimized below comparison\"\"\"\n",
    "\n",
    "    @PerformanceProfiler.profile_execution\n",
    "    def compare(self, df: pd.DataFrame, x: str, y: Union[str, float],\n",
    "                new_col: Optional[str] = None) -> pd.DataFrame:\n",
    "\n",
    "        self._validate_inputs(df, x, y)\n",
    "\n",
    "        if isinstance(y, (int, float)):\n",
    "            comparison_array = df[x].values < y\n",
    "        else:\n",
    "            comparison_array = df[x].values < df[y].values\n",
    "\n",
    "        new_col = new_col or self._generate_column_name(x, y, \"below\")\n",
    "        df[new_col] = comparison_array.astype(np.int8)\n",
    "\n",
    "        return df\n",
    "\n",
    "class CrossedUpComparator(BaseComparator):\n",
    "    \"\"\"Optimized crossed up detection\"\"\"\n",
    "\n",
    "    @PerformanceProfiler.profile_execution\n",
    "    def compare(self, df: pd.DataFrame, x: str, y: Union[str, float],\n",
    "                new_col: Optional[str] = None) -> pd.DataFrame:\n",
    "\n",
    "        self._validate_inputs(df, x, y)\n",
    "\n",
    "        x_values = df[x].values\n",
    "\n",
    "        if isinstance(y, (int, float)):\n",
    "            diff = x_values - y\n",
    "            diff_prev = np.roll(diff, 1)\n",
    "        else:\n",
    "            y_values = df[y].values\n",
    "            diff = x_values - y_values\n",
    "            diff_prev = np.roll(diff, 1)\n",
    "\n",
    "        # Vectorized cross-up detection\n",
    "        crossed_up = (diff > 0) & (diff_prev <= 0)\n",
    "        crossed_up[0] = False  # First element cannot be a cross\n",
    "\n",
    "        new_col = new_col or self._generate_column_name(x, y, \"crossed_up\")\n",
    "        df[new_col] = crossed_up.astype(np.int8)\n",
    "\n",
    "        return df\n",
    "\n",
    "class CrossedDownComparator(BaseComparator):\n",
    "    \"\"\"Optimized crossed down detection\"\"\"\n",
    "\n",
    "    @PerformanceProfiler.profile_execution\n",
    "    def compare(self, df: pd.DataFrame, x: str, y: Union[str, float],\n",
    "                new_col: Optional[str] = None) -> pd.DataFrame:\n",
    "\n",
    "        self._validate_inputs(df, x, y)\n",
    "\n",
    "        x_values = df[x].values\n",
    "\n",
    "        if isinstance(y, (int, float)):\n",
    "            diff = x_values - y\n",
    "            diff_prev = np.roll(diff, 1)\n",
    "        else:\n",
    "            y_values = df[y].values\n",
    "            diff = x_values - y_values\n",
    "            diff_prev = np.roll(diff, 1)\n",
    "\n",
    "        crossed_down = (diff < 0) & (diff_prev >= 0)\n",
    "        crossed_down[0] = False\n",
    "\n",
    "        new_col = new_col or self._generate_column_name(x, y, \"crossed_down\")\n",
    "        df[new_col] = crossed_down.astype(np.int8)\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e56dee-45b8-4e0f-a22e-257ea9a8991b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComparatorFactory:\n",
    "    \"\"\"Enhanced factory with configuration injection\"\"\"\n",
    "\n",
    "    def __init__(self, config: TechnicalAnalysisConfig):\n",
    "        self.config = config\n",
    "        self._comparators: Dict[str, BaseComparator] = {\n",
    "            ComparisonType.ABOVE.value: AboveComparator(config),\n",
    "            ComparisonType.BELOW.value: BelowComparator(config),\n",
    "            ComparisonType.CROSSED_UP.value: CrossedUpComparator(config),\n",
    "            ComparisonType.CROSSED_DOWN.value: CrossedDownComparator(config),\n",
    "        }\n",
    "\n",
    "    def get_comparator(self, operation: str) -> BaseComparator:\n",
    "        \"\"\"Get comparator instance for the given operation\"\"\"\n",
    "        comparator = self._comparators.get(operation.lower())\n",
    "        if not comparator:\n",
    "            available = list(self._comparators.keys())\n",
    "            raise TAException(\n",
    "                f\"Unsupported operation '{operation}'\",\n",
    "                \"UNSUPPORTED_OPERATION\",\n",
    "                {\"available_operations\": available}\n",
    "            )\n",
    "        return comparator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbfe0c9-943d-4345-8bcb-3f92d33c9390",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryParser:\n",
    "    \"\"\"Enhanced query parser with better pattern matching\"\"\"\n",
    "\n",
    "    COMPARISON_PATTERNS = {\n",
    "        r'\\babove\\b': ComparisonType.ABOVE.value,\n",
    "        r'\\bbelow\\b': ComparisonType.BELOW.value,\n",
    "        r'\\bcrossed[\\s_]?up\\b': ComparisonType.CROSSED_UP.value,\n",
    "        r'\\bcrossed[\\s_]?down\\b': ComparisonType.CROSSED_DOWN.value,\n",
    "        r'\\bequals?\\b': ComparisonType.EQUALS.value,\n",
    "        r'\\bgreater[\\s_]?than[\\s_]?or[\\s_]?equal\\b': ComparisonType.GREATER_EQUAL.value,\n",
    "        r'\\bless[\\s_]?than[\\s_]?or[\\s_]?equal\\b': ComparisonType.LESS_EQUAL.value,\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def parse_query(cls, query: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Enhanced query parsing with better error recovery\"\"\"\n",
    "        operations = []\n",
    "\n",
    "        for line_num, line in enumerate(query.strip().splitlines(), 1):\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith('#'):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                operation = cls._parse_line(line)\n",
    "                if operation:\n",
    "                    operation['line_number'] = line_num\n",
    "                    operations.append(operation)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error parsing line {line_num}: '{line}' - {e}\")\n",
    "                continue\n",
    "\n",
    "        return operations\n",
    "\n",
    "    @classmethod\n",
    "    def _parse_line(cls, line: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Parse a single query line with better error handling\"\"\"\n",
    "        line_lower = line.lower()\n",
    "\n",
    "        # Find comparison operation\n",
    "        comparison = None\n",
    "        for pattern, comp_type in cls.COMPARISON_PATTERNS.items():\n",
    "            if re.search(pattern, line_lower):\n",
    "                comparison = comp_type\n",
    "                break\n",
    "\n",
    "        if not comparison:\n",
    "            logger.warning(f\"No valid comparison found in: {line}\")\n",
    "            return None\n",
    "\n",
    "        # Split by comparison operation\n",
    "        parts = re.split(r'\\b(?:above|below|crossed[\\s_]?(?:up|down)|equals?|greater[\\s_]?than[\\s_]?or[\\s_]?equal|less[\\s_]?than[\\s_]?or[\\s_]?equal)\\b',\n",
    "                        line, flags=re.IGNORECASE)\n",
    "\n",
    "        if len(parts) < 2:\n",
    "            logger.warning(f\"Malformed query line: {line}\")\n",
    "            return None\n",
    "\n",
    "        column1 = parts[0].strip()\n",
    "        column2 = parts[1].strip()\n",
    "\n",
    "        # Try to convert column2 to numeric\n",
    "        try:\n",
    "            column2 = float(column2)\n",
    "        except ValueError:\n",
    "            pass  # Keep as string\n",
    "\n",
    "        return {\n",
    "            'column1': column1,\n",
    "            'operation': comparison,\n",
    "            'column2': column2,\n",
    "            'original_line': line\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_indicators(query: str) -> List[IndicatorConfig]:\n",
    "        \"\"\"Extract indicator configurations from query with better parsing\"\"\"\n",
    "        indicators = []\n",
    "        # Find indicator patterns like EMA_21, RSI_14, etc.\n",
    "        indicator_patterns = re.findall(r'\\b([A-Z]+)_(\\d+)\\b', query.upper())\n",
    "\n",
    "        for name, period in indicator_patterns:\n",
    "            if name not in ['ABOVE', 'BELOW', 'CROSSED', 'UP', 'DOWN', 'EQUALS']:\n",
    "                indicators.append(IndicatorConfig(name=name, period=int(period)))\n",
    "\n",
    "        # Find standalone indicators\n",
    "        standalone_indicators = re.findall(r'\\b([A-Z]{3,})\\b', query.upper())\n",
    "        for name in standalone_indicators:\n",
    "            if name not in ['ABOVE', 'BELOW', 'CROSSED', 'DOWN', 'EQUALS'] and len(name) >= 3:\n",
    "                # Check if it's not already added with a period\n",
    "                if not any(ind.name == name for ind in indicators):\n",
    "                    indicators.append(IndicatorConfig(name=name))\n",
    "\n",
    "        # Remove duplicates while preserving order\n",
    "        unique_indicators = []\n",
    "        seen = set()\n",
    "        for ind in indicators:\n",
    "            key = (ind.name, ind.period)\n",
    "            if key not in seen:\n",
    "                unique_indicators.append(ind)\n",
    "                seen.add(key)\n",
    "\n",
    "        return unique_indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c529ab-755b-45ec-bff4-2417a7373ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignalGenerator:\n",
    "    \"\"\"Separated signal generation logic\"\"\"\n",
    "\n",
    "    def __init__(self, config: TechnicalAnalysisConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def combine_signals(self, df: pd.DataFrame, signal_columns: List[str],\n",
    "                       output_column: str, operation: str = 'AND') -> pd.DataFrame:\n",
    "        \"\"\"Combine multiple signal columns with specified operation\"\"\"\n",
    "        available_signals = [col for col in signal_columns if col in df.columns]\n",
    "\n",
    "        if not available_signals:\n",
    "            logger.warning(f\"No signal columns found for combination: {signal_columns}\")\n",
    "            return df\n",
    "\n",
    "        if operation.upper() == 'AND':\n",
    "            combined = df[available_signals[0]].astype(bool)\n",
    "            for signal in available_signals[1:]:\n",
    "                combined = combined & df[signal].astype(bool)\n",
    "        elif operation.upper() == 'OR':\n",
    "            combined = df[available_signals[0]].astype(bool)\n",
    "            for signal in available_signals[1:]:\n",
    "                combined = combined | df[signal].astype(bool)\n",
    "        else:\n",
    "            raise TAException(f\"Unsupported signal combination operation: {operation}\")\n",
    "\n",
    "        df[output_column] = combined.astype(np.int8)\n",
    "        logger.info(f\"Combined {len(available_signals)} signals into '{output_column}' using {operation}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def generate_trend_following_signals(self, df: pd.DataFrame,\n",
    "                                       indicator_engine: TALibIndicatorEngine) -> pd.DataFrame:\n",
    "        \"\"\"Generate trend-following signals with robust error handling\"\"\"\n",
    "\n",
    "        # Required indicators\n",
    "        indicators = [\n",
    "            IndicatorConfig(name='EMA', period=21),\n",
    "            IndicatorConfig(name='EMA', period=50),\n",
    "            IndicatorConfig(name='RSI', period=14),\n",
    "        ]\n",
    "\n",
    "        # Add indicators\n",
    "        for indicator in indicators:\n",
    "            try:\n",
    "                column_name = f\"{indicator.name}_{indicator.period}\"\n",
    "                if column_name not in df.columns:\n",
    "                    result = indicator_engine.calculate_indicator(df, indicator)\n",
    "                    df[column_name] = result\n",
    "                    logger.info(f\"Added indicator: {column_name}\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not add {indicator.name}_{indicator.period}: {e}\")\n",
    "                continue\n",
    "\n",
    "        return df\n",
    "\n",
    "    def generate_mean_reversion_signals(self, df: pd.DataFrame,\n",
    "                                      indicator_engine: TALibIndicatorEngine) -> pd.DataFrame:\n",
    "        \"\"\"Generate mean reversion signals\"\"\"\n",
    "\n",
    "        try:\n",
    "            # Add RSI\n",
    "            rsi_config = IndicatorConfig(name='RSI', period=14)\n",
    "            if 'RSI_14' not in df.columns:\n",
    "                df['RSI_14'] = indicator_engine.calculate_indicator(df, rsi_config)\n",
    "\n",
    "            # Add simple moving average for Bollinger Bands middle\n",
    "            sma_config = IndicatorConfig(name='SMA', period=self.config.DEFAULT_BB_PERIOD)\n",
    "            if f'SMA_{self.config.DEFAULT_BB_PERIOD}' not in df.columns:\n",
    "                df[f'SMA_{self.config.DEFAULT_BB_PERIOD}'] = indicator_engine.calculate_indicator(df, sma_config)\n",
    "\n",
    "            # Calculate Bollinger Bands manually\n",
    "            close_prices = df['Close']\n",
    "            sma_col = f'SMA_{self.config.DEFAULT_BB_PERIOD}'\n",
    "            if sma_col in df.columns:\n",
    "                sma = df[sma_col]\n",
    "                rolling_std = close_prices.rolling(window=self.config.DEFAULT_BB_PERIOD).std()\n",
    "\n",
    "                df['BB_UPPER'] = sma + (self.config.DEFAULT_BB_STD * rolling_std)\n",
    "                df['BB_LOWER'] = sma - (self.config.DEFAULT_BB_STD * rolling_std)\n",
    "                df['BB_MIDDLE'] = sma\n",
    "\n",
    "                logger.info(\"Generated Bollinger Bands\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not generate mean reversion indicators: {e}\")\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a6b0c2-8e2c-4222-b2ba-a3e1ca618e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceAnalyzer:\n",
    "    \"\"\"Separated performance analysis and backtesting\"\"\"\n",
    "\n",
    "    def __init__(self, config: TechnicalAnalysisConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def backtest_signals(self, df: pd.DataFrame, signal_column: str,\n",
    "                        entry_price_column: str = 'Close',\n",
    "                        holding_period: int = 1) -> Dict[str, float]:\n",
    "        \"\"\"Enhanced backtesting with more metrics\"\"\"\n",
    "\n",
    "        if signal_column not in df.columns:\n",
    "            raise TAException(f\"Signal column '{signal_column}' not found\")\n",
    "\n",
    "        if entry_price_column not in df.columns:\n",
    "            raise TAException(f\"Price column '{entry_price_column}' not found\")\n",
    "\n",
    "        signals = df[signal_column]\n",
    "        prices = df[entry_price_column]\n",
    "\n",
    "        # Find signal entry points\n",
    "        entry_points = np.where(signals == 1)[0]\n",
    "\n",
    "        if len(entry_points) == 0:\n",
    "            return {\n",
    "                'total_signals': 0,\n",
    "                'total_trades': 0,\n",
    "                'win_rate': 0.0,\n",
    "                'avg_return': 0.0,\n",
    "                'total_return': 0.0,\n",
    "                'max_drawdown': 0.0,\n",
    "                'sharpe_ratio': 0.0\n",
    "            }\n",
    "\n",
    "        returns = []\n",
    "        equity_curve = []\n",
    "\n",
    "        for entry_idx in entry_points:\n",
    "            exit_idx = min(entry_idx + holding_period, len(prices) - 1)\n",
    "\n",
    "            if exit_idx > entry_idx:\n",
    "                entry_price = prices.iloc[entry_idx]\n",
    "                exit_price = prices.iloc[exit_idx]\n",
    "\n",
    "                if entry_price != 0:  # Avoid division by zero\n",
    "                    ret = (exit_price - entry_price) / entry_price\n",
    "                    returns.append(ret)\n",
    "                    equity_curve.append(1 + ret if not equity_curve else equity_curve[-1] * (1 + ret))\n",
    "\n",
    "        if not returns:\n",
    "            return {\n",
    "                'total_signals': len(entry_points),\n",
    "                'total_trades': 0,\n",
    "                'win_rate': 0.0,\n",
    "                'avg_return': 0.0,\n",
    "                'total_return': 0.0,\n",
    "                'max_drawdown': 0.0,\n",
    "                'sharpe_ratio': 0.0\n",
    "            }\n",
    "\n",
    "        returns_array = np.array(returns)\n",
    "\n",
    "        # Calculate metrics\n",
    "        win_rate = (returns_array > 0).sum() / len(returns_array)\n",
    "        avg_return = returns_array.mean()\n",
    "        total_return = returns_array.sum()\n",
    "\n",
    "        # Calculate max drawdown\n",
    "        equity_curve = np.array(equity_curve)\n",
    "        running_max = np.maximum.accumulate(equity_curve)\n",
    "        drawdown = (equity_curve - running_max) / running_max\n",
    "        max_drawdown = drawdown.min()\n",
    "\n",
    "        # Calculate Sharpe ratio (assuming 252 trading days)\n",
    "        sharpe_ratio = (avg_return * 252) / (returns_array.std() * np.sqrt(252)) if returns_array.std() > 0 else 0\n",
    "\n",
    "        return {\n",
    "            'total_signals': len(entry_points),\n",
    "            'total_trades': len(returns),\n",
    "            'win_rate': win_rate,\n",
    "            'avg_return': avg_return,\n",
    "            'total_return': total_return,\n",
    "            'best_return': returns_array.max(),\n",
    "            'worst_return': returns_array.min(),\n",
    "            'max_drawdown': max_drawdown,\n",
    "            'sharpe_ratio': sharpe_ratio,\n",
    "            'volatility': returns_array.std()\n",
    "        }\n",
    "\n",
    "    def generate_performance_report(self, operations_log: List[AnalysisResult]) -> Dict[str, Any]:\n",
    "        \"\"\"Generate comprehensive performance report\"\"\"\n",
    "\n",
    "        total_operations = len(operations_log)\n",
    "        successful_operations = sum(1 for op in operations_log if op.success)\n",
    "        total_execution_time = sum(op.execution_time for op in operations_log)\n",
    "\n",
    "        return {\n",
    "            'summary': {\n",
    "                'total_operations': total_operations,\n",
    "                'successful_operations': successful_operations,\n",
    "                'success_rate': (successful_operations / total_operations * 100) if total_operations > 0 else 0,\n",
    "                'total_execution_time_ms': total_execution_time * 1000,\n",
    "                'avg_execution_time_ms': (total_execution_time / total_operations * 1000) if total_operations > 0 else 0\n",
    "            },\n",
    "            'operation_details': [\n",
    "                {\n",
    "                    'operation': op.operation,\n",
    "                    'success': op.success,\n",
    "                    'execution_time_ms': op.execution_time * 1000,\n",
    "                    'message': op.message\n",
    "                } for op in operations_log\n",
    "            ],\n",
    "            'failed_operations': [\n",
    "                op.operation for op in operations_log if not op.success\n",
    "            ]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eda5d7c-c7ee-4c6e-bf95-81ccf47c8cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedTechnicalAnalyzer:\n",
    "    \"\"\"Main analyzer class with clean architecture and separated concerns\"\"\"\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame,\n",
    "                 config: Optional[TechnicalAnalysisConfig] = None,\n",
    "                 validate_ohlcv: bool = True):\n",
    "        \"\"\"Initialize with dependency injection and clean architecture\"\"\"\n",
    "\n",
    "        # Configuration\n",
    "        self.config = config or TechnicalAnalysisConfig()\n",
    "\n",
    "        # Core components\n",
    "        self.cache_manager = CacheManager(self.config)\n",
    "        self.data_manager = DataManager(self.config)\n",
    "        self.indicator_engine = TALibIndicatorEngine(self.config, self.cache_manager)\n",
    "        self.comparator_factory = ComparatorFactory(self.config)\n",
    "        self.signal_generator = SignalGenerator(self.config)\n",
    "        self.performance_analyzer = PerformanceAnalyzer(self.config)\n",
    "\n",
    "        # Data preparation\n",
    "        self._original_df = df.copy()\n",
    "        self._df = self.data_manager.prepare_dataframe(df, validate_ohlcv)\n",
    "        self._operations_log: List[AnalysisResult] = []\n",
    "\n",
    "        # Log initialization\n",
    "        memory_info = self.data_manager.get_memory_usage(self._df)\n",
    "        logger.info(f\"Initialized analyzer - Shape: {self._df.shape}, Memory: {memory_info['total_mb']:.2f}MB\")\n",
    "\n",
    "    @property\n",
    "    def df(self) -> pd.DataFrame:\n",
    "        \"\"\"Get the current DataFrame\"\"\"\n",
    "        return self._df\n",
    "\n",
    "    @property\n",
    "    def operations_log(self) -> List[AnalysisResult]:\n",
    "        \"\"\"Get log of all operations performed\"\"\"\n",
    "        return self._operations_log\n",
    "\n",
    "    def add_indicator(self, config: IndicatorConfig,\n",
    "                     column_name: Optional[str] = None) -> 'EnhancedTechnicalAnalyzer':\n",
    "        \"\"\"Add technical indicator with enhanced error handling\"\"\"\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        try:\n",
    "            result = self.indicator_engine.calculate_indicator(self._df, config)\n",
    "\n",
    "            # Generate column name\n",
    "            if column_name is None:\n",
    "                if config.period:\n",
    "                    column_name = f\"{config.name}_{config.period}\"\n",
    "                else:\n",
    "                    column_name = config.name\n",
    "\n",
    "            self._df[column_name] = result\n",
    "            execution_time = time.perf_counter() - start_time\n",
    "\n",
    "            # Log successful operation\n",
    "            analysis_result = AnalysisResult(\n",
    "                column_name=column_name,\n",
    "                operation=f\"ADD_INDICATOR_{config.name}\",\n",
    "                success=True,\n",
    "                message=\"Indicator added successfully\",\n",
    "                data=result,\n",
    "                execution_time=execution_time\n",
    "            )\n",
    "            self._operations_log.append(analysis_result)\n",
    "\n",
    "            logger.info(f\"✓ Added indicator {column_name} in {execution_time:.4f}s\")\n",
    "\n",
    "        except Exception as e:\n",
    "            execution_time = time.perf_counter() - start_time\n",
    "            analysis_result = AnalysisResult(\n",
    "                column_name=column_name or config.name,\n",
    "                operation=f\"ADD_INDICATOR_{config.name}\",\n",
    "                success=False,\n",
    "                message=str(e),\n",
    "                execution_time=execution_time\n",
    "            )\n",
    "            self._operations_log.append(analysis_result)\n",
    "            logger.error(f\"✗ Failed to add indicator {config.name}: {e}\")\n",
    "            raise TAException(f\"Failed to add indicator: {e}\", \"INDICATOR_ADD_FAILED\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    # Fluent interface methods\n",
    "    def above(self, x: str, y: Union[str, float], new_col: Optional[str] = None) -> 'EnhancedTechnicalAnalyzer':\n",
    "        \"\"\"Fluent interface for above comparison\"\"\"\n",
    "        return self._execute_comparison(ComparisonType.ABOVE.value, x, y, new_col)\n",
    "\n",
    "    def below(self, x: str, y: Union[str, float], new_col: Optional[str] = None) -> 'EnhancedTechnicalAnalyzer':\n",
    "        \"\"\"Fluent interface for below comparison\"\"\"\n",
    "        return self._execute_comparison(ComparisonType.BELOW.value, x, y, new_col)\n",
    "\n",
    "    def crossed_up(self, x: str, y: Union[str, float], new_col: Optional[str] = None) -> 'EnhancedTechnicalAnalyzer':\n",
    "        \"\"\"Fluent interface for crossed up detection\"\"\"\n",
    "        return self._execute_comparison(ComparisonType.CROSSED_UP.value, x, y, new_col)\n",
    "\n",
    "    def crossed_down(self, x: str, y: Union[str, float], new_col: Optional[str] = None) -> 'EnhancedTechnicalAnalyzer':\n",
    "        \"\"\"Fluent interface for crossed down detection\"\"\"\n",
    "        return self._execute_comparison(ComparisonType.CROSSED_DOWN.value, x, y, new_col)\n",
    "\n",
    "    @PerformanceProfiler.profile_execution\n",
    "    def _execute_comparison(self, operation: str, x: str, y: Union[str, float],\n",
    "                          new_col: Optional[str] = None) -> 'EnhancedTechnicalAnalyzer':\n",
    "        \"\"\"Execute comparison operation with enhanced error handling\"\"\"\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        try:\n",
    "            comparator = self.comparator_factory.get_comparator(operation)\n",
    "            self._df = comparator.compare(self._df, x, y, new_col)\n",
    "\n",
    "            execution_time = time.perf_counter() - start_time\n",
    "            result_col = new_col or comparator._generate_column_name(x, y, operation)\n",
    "\n",
    "            result = AnalysisResult(\n",
    "                column_name=result_col,\n",
    "                operation=f\"{x} {operation} {y}\",\n",
    "                success=True,\n",
    "                message=\"Comparison completed successfully\",\n",
    "                data=self._df[result_col],\n",
    "                execution_time=execution_time\n",
    "            )\n",
    "            self._operations_log.append(result)\n",
    "            logger.info(f\"✓ {result.operation} -> {result.column_name} ({execution_time:.4f}s)\")\n",
    "\n",
    "        except Exception as e:\n",
    "            execution_time = time.perf_counter() - start_time\n",
    "            result = AnalysisResult(\n",
    "                column_name=\"\",\n",
    "                operation=f\"{x} {operation} {y}\",\n",
    "                success=False,\n",
    "                message=str(e),\n",
    "                execution_time=execution_time\n",
    "            )\n",
    "            self._operations_log.append(result)\n",
    "            logger.error(f\"✗ {result.operation}: {result.message}\")\n",
    "            raise TAException(f\"Comparison operation failed: {e}\", \"COMPARISON_FAILED\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def execute_query(self, query: str, auto_add_indicators: bool = True) -> 'EnhancedTechnicalAnalyzer':\n",
    "        \"\"\"Execute natural language query with automatic indicator addition\"\"\"\n",
    "\n",
    "        try:\n",
    "            # Extract and add indicators if requested\n",
    "            if auto_add_indicators:\n",
    "                indicators = QueryParser.extract_indicators(query)\n",
    "                logger.info(f\"Found {len(indicators)} indicators in query\")\n",
    "\n",
    "                for indicator_config in indicators:\n",
    "                    try:\n",
    "                        self.add_indicator(indicator_config)\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Could not add indicator {indicator_config.name}: {e}\")\n",
    "\n",
    "            # Parse and execute comparisons\n",
    "            operations = QueryParser.parse_query(query)\n",
    "            logger.info(f\"Parsed {len(operations)} operations from query\")\n",
    "\n",
    "            successful_operations = 0\n",
    "            for op in operations:\n",
    "                try:\n",
    "                    self._execute_comparison(op['operation'], op['column1'], op['column2'])\n",
    "                    successful_operations += 1\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Failed to execute query operation {op}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            logger.info(f\"Successfully executed {successful_operations}/{len(operations)} operations\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Query execution failed: {e}\")\n",
    "            raise TAException(f\"Query execution failed: {e}\", \"QUERY_EXECUTION_FAILED\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def generate_trend_following_signals(self) -> 'EnhancedTechnicalAnalyzer':\n",
    "        \"\"\"Generate trend-following signals using signal generator\"\"\"\n",
    "        self._df = self.signal_generator.generate_trend_following_signals(self._df, self.indicator_engine)\n",
    "\n",
    "        # Create combined signal if components are available\n",
    "        signal_components = ['Close_above_EMA_21', 'EMA_21_above_EMA_50', 'RSI_14_below_70_0']\n",
    "        available_signals = [col for col in signal_components if col in self._df.columns]\n",
    "\n",
    "        if len(available_signals) >= 2:\n",
    "            self._df = self.signal_generator.combine_signals(\n",
    "                self._df, available_signals, 'trend_following_signal', 'AND'\n",
    "            )\n",
    "\n",
    "        return self\n",
    "\n",
    "    def generate_mean_reversion_signals(self) -> 'EnhancedTechnicalAnalyzer':\n",
    "        \"\"\"Generate mean reversion signals using signal generator\"\"\"\n",
    "        self._df = self.signal_generator.generate_mean_reversion_signals(self._df, self.indicator_engine)\n",
    "\n",
    "        # Create combined signal if components are available\n",
    "        signal_components = ['Close_below_BB_LOWER', 'RSI_14_below_30_0']\n",
    "        available_signals = [col for col in signal_components if col in self._df.columns]\n",
    "\n",
    "        if len(available_signals) >= 1:\n",
    "            self._df = self.signal_generator.combine_signals(\n",
    "                self._df, available_signals, 'mean_reversion_signal', 'AND'\n",
    "            )\n",
    "\n",
    "        return self\n",
    "\n",
    "    def get_signals(self, column: str) -> pd.Series:\n",
    "        \"\"\"Get signal series for a specific column\"\"\"\n",
    "        if column not in self._df.columns:\n",
    "            raise TAException(f\"Column '{column}' not found\", \"COLUMN_NOT_FOUND\")\n",
    "        return self._df[column]\n",
    "\n",
    "    def get_active_signals(self, column: str, include_index: bool = True) -> pd.DataFrame:\n",
    "        \"\"\"Get only rows where signal is active\"\"\"\n",
    "        if column not in self._df.columns:\n",
    "            raise TAException(f\"Column '{column}' not found\", \"COLUMN_NOT_FOUND\")\n",
    "\n",
    "        active_mask = self._df[column] == 1\n",
    "        result_df = self._df[active_mask]\n",
    "\n",
    "        return result_df if include_index else result_df.reset_index(drop=True)\n",
    "\n",
    "    def summary(self) -> pd.DataFrame:\n",
    "        \"\"\"Enhanced summary with performance metrics\"\"\"\n",
    "        summary_data = []\n",
    "        for result in self._operations_log:\n",
    "            summary_data.append({\n",
    "                'Operation': result.operation,\n",
    "                'Column': result.column_name,\n",
    "                'Success': result.success,\n",
    "                'Execution_Time_ms': round(result.execution_time * 1000, 2),\n",
    "                'Active_Signals': result.data.sum() if result.data is not None else 0,\n",
    "                'Signal_Ratio_%': round((result.data.sum() / len(result.data) * 100) if result.data is not None else 0, 2),\n",
    "                'Message': result.message\n",
    "            })\n",
    "        return pd.DataFrame(summary_data)\n",
    "\n",
    "    def performance_report(self) -> Dict[str, Any]:\n",
    "        \"\"\"Generate comprehensive performance report\"\"\"\n",
    "        base_report = self.performance_analyzer.generate_performance_report(self._operations_log)\n",
    "\n",
    "        # Add memory and cache statistics\n",
    "        memory_info = self.data_manager.get_memory_usage(self._df)\n",
    "        cache_stats = self.cache_manager.get_stats()\n",
    "\n",
    "        base_report.update({\n",
    "            'memory_usage': memory_info,\n",
    "            'cache_statistics': cache_stats,\n",
    "            'dataframe_info': {\n",
    "                'shape': self._df.shape,\n",
    "                'generated_columns': len([col for col in self._df.columns if col not in self._original_df.columns]),\n",
    "                'total_columns': len(self._df.columns)\n",
    "            }\n",
    "        })\n",
    "\n",
    "        return base_report\n",
    "\n",
    "    def backtest_signals(self, signal_column: str, entry_price_column: str = 'Close',\n",
    "                        holding_period: int = 1) -> Dict[str, float]:\n",
    "        \"\"\"Backtest signals using performance analyzer\"\"\"\n",
    "        return self.performance_analyzer.backtest_signals(\n",
    "            self._df, signal_column, entry_price_column, holding_period\n",
    "        )\n",
    "\n",
    "    def reset(self) -> 'EnhancedTechnicalAnalyzer':\n",
    "        \"\"\"Reset to original DataFrame state with cache clearing\"\"\"\n",
    "        self._df = self.data_manager.prepare_dataframe(self._original_df)\n",
    "        self._operations_log.clear()\n",
    "        self.cache_manager.clear()\n",
    "        logger.info(\"Reset analyzer to original state\")\n",
    "        return self\n",
    "\n",
    "    def export_signals(self, filename: str, format: str = 'csv',\n",
    "                      columns: Optional[List[str]] = None) -> bool:\n",
    "        \"\"\"Export signals with enhanced options\"\"\"\n",
    "        try:\n",
    "            if columns is None:\n",
    "                # Auto-detect signal columns\n",
    "                signal_columns = [col for col in self._df.columns\n",
    "                                if any(op in col.lower() for op in ['above', 'below', 'crossed', 'signal'])]\n",
    "                columns = signal_columns + ['Close']\n",
    "\n",
    "            export_df = self._df[columns].copy()\n",
    "\n",
    "            if format.lower() == 'csv':\n",
    "                export_df.to_csv(filename, index=True)\n",
    "            elif format.lower() == 'parquet':\n",
    "                export_df.to_parquet(filename, index=True)\n",
    "            elif format.lower() == 'excel':\n",
    "                export_df.to_excel(filename, index=True)\n",
    "            else:\n",
    "                raise TAException(f\"Unsupported export format: {format}\", \"UNSUPPORTED_FORMAT\")\n",
    "\n",
    "            logger.info(f\"Exported {len(columns)} columns to {filename}\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Export failed: {e}\")\n",
    "            return False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-chartter-v2-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
