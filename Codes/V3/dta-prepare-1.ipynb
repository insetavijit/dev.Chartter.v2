{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c335c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"HI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f41d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fileName = 'DAT_ASCII_XAUUSD_M1_2024'\n",
    "# Correct way to read a Parquet file\n",
    "ohlc = pd.read_parquet(\"./src/DAT_ASCII_XAUUSD_M1_2024.parquet\")\n",
    "\n",
    "# Optional: check the first few rows\n",
    "print(ohlc.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32242656",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ohlc.tail(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0e25a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Enterprise-Level Data Resampling System\n",
    "======================================\n",
    "\n",
    "A comprehensive, production-ready system for resampling financial time series data\n",
    "with advanced features including validation, logging, caching, and extensibility.\n",
    "\n",
    "Author: Enterprise Data Team\n",
    "Version: 1.0.0\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from typing import Dict, List, Optional, Union, Callable, Any\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "from abc import ABC, abstractmethod\n",
    "from functools import wraps\n",
    "import hashlib\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "def setup_logging(log_level: str = \"INFO\", log_file: Optional[str] = None) -> logging.Logger:\n",
    "    \"\"\"Setup enterprise logging configuration.\"\"\"\n",
    "    logger = logging.getLogger(\"DataResampler\")\n",
    "    logger.setLevel(getattr(logging, log_level.upper()))\n",
    "\n",
    "    # Clear existing handlers\n",
    "    for handler in logger.handlers[:]:\n",
    "        logger.removeHandler(handler)\n",
    "\n",
    "    # Create formatter\n",
    "    formatter = logging.Formatter(\n",
    "        '%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s'\n",
    "    )\n",
    "\n",
    "    # Console handler\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(formatter)\n",
    "    logger.addHandler(console_handler)\n",
    "\n",
    "    # File handler if specified\n",
    "    if log_file:\n",
    "        file_handler = logging.FileHandler(log_file)\n",
    "        file_handler.setFormatter(formatter)\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "\n",
    "# Custom Exceptions\n",
    "class DataResamplerError(Exception):\n",
    "    \"\"\"Base exception for data resampling operations.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class ValidationError(DataResamplerError):\n",
    "    \"\"\"Raised when data validation fails.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class ConfigurationError(DataResamplerError):\n",
    "    \"\"\"Raised when configuration is invalid.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class ProcessingError(DataResamplerError):\n",
    "    \"\"\"Raised when data processing fails.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# Configuration Management\n",
    "@dataclass\n",
    "class ResamplingConfig:\n",
    "    \"\"\"Configuration class for resampling operations.\"\"\"\n",
    "\n",
    "    # Core resampling settings\n",
    "    default_period: str = '4H'\n",
    "    supported_periods: List[str] = field(default_factory=lambda: [\n",
    "        '1T', '5T', '15T', '30T', '1H', '2H', '4H', '6H', '12H', '1D', '1W', '1M'\n",
    "    ])\n",
    "\n",
    "    # Data validation settings\n",
    "    required_columns: List[str] = field(default_factory=lambda: [\n",
    "        'datetime', 'open', 'high', 'low', 'close', 'vol'\n",
    "    ])\n",
    "    numeric_columns: List[str] = field(default_factory=lambda: [\n",
    "        'open', 'high', 'low', 'close', 'vol'\n",
    "    ])\n",
    "\n",
    "    # Processing settings\n",
    "    drop_na_periods: bool = True\n",
    "    validate_ohlc: bool = True\n",
    "    min_data_points: int = 1\n",
    "    max_gap_tolerance: str = '1D'\n",
    "\n",
    "    # Performance settings\n",
    "    enable_caching: bool = True\n",
    "    cache_size_limit: int = 100  # Number of cached results\n",
    "    parallel_processing: bool = False\n",
    "    max_workers: int = 4\n",
    "\n",
    "    # Output settings\n",
    "    reset_index: bool = True\n",
    "    datetime_column_name: str = 'datetime'\n",
    "\n",
    "    @classmethod\n",
    "    def from_file(cls, config_path: Union[str, Path]) -> 'ResamplingConfig':\n",
    "        \"\"\"Load configuration from JSON file.\"\"\"\n",
    "        try:\n",
    "            with open(config_path, 'r') as f:\n",
    "                config_dict = json.load(f)\n",
    "            return cls(**config_dict)\n",
    "        except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "            raise ConfigurationError(f\"Failed to load configuration: {e}\")\n",
    "\n",
    "    def save_to_file(self, config_path: Union[str, Path]) -> None:\n",
    "        \"\"\"Save configuration to JSON file.\"\"\"\n",
    "        config_dict = {\n",
    "            k: v for k, v in self.__dict__.items()\n",
    "            if not k.startswith('_')\n",
    "        }\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(config_dict, f, indent=2, default=str)\n",
    "\n",
    "\n",
    "# Date Filtering System\n",
    "class EnterpriseDateFilter:\n",
    "    \"\"\"Enterprise-grade date filtering with comprehensive validation and error handling.\"\"\"\n",
    "\n",
    "    def __init__(self, logger: logging.Logger):\n",
    "        self.logger = logger\n",
    "        self.metrics = {\n",
    "            'filter_operations': 0,\n",
    "            'rows_filtered': 0,\n",
    "            'invalid_dates_found': 0\n",
    "        }\n",
    "\n",
    "    def filter_by_date_range(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        start_date: Union[str, datetime],\n",
    "        end_date: Union[str, datetime],\n",
    "        datetime_col: str = 'datetime',\n",
    "        inclusive: str = 'both',\n",
    "        date_format: Optional[str] = None,\n",
    "        column_date_format: Optional[str] = None,\n",
    "        handle_invalid: str = 'warn'  # 'warn', 'error', 'ignore'\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Enterprise-grade date filtering with comprehensive validation and error handling.\n",
    "\n",
    "        Parameters:\n",
    "            df (pd.DataFrame): Input DataFrame containing the datetime column.\n",
    "            start_date (str or datetime): Start of the date range.\n",
    "            end_date (str or datetime): End of the date range.\n",
    "            datetime_col (str): Name of the datetime column in df.\n",
    "            inclusive (str): Boundary inclusion behavior ('both', 'left', 'right', 'neither').\n",
    "            date_format (str, optional): Format of input date strings.\n",
    "            column_date_format (str, optional): Format of the datetime column.\n",
    "            handle_invalid (str): How to handle invalid dates ('warn', 'error', 'ignore').\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Filtered DataFrame containing rows within the specified date range.\n",
    "\n",
    "        Raises:\n",
    "            ValidationError: If inputs are invalid or datetime conversion fails.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        original_rows = len(df)\n",
    "\n",
    "        try:\n",
    "            # Input validation\n",
    "            self._validate_filter_inputs(df, datetime_col, inclusive, handle_invalid)\n",
    "\n",
    "            # Convert and validate date range\n",
    "            start_dt, end_dt = self._parse_and_validate_dates(\n",
    "                start_date, end_date, date_format\n",
    "            )\n",
    "\n",
    "            # Process datetime column\n",
    "            df = self._process_datetime_column(\n",
    "                df, datetime_col, column_date_format, handle_invalid\n",
    "            )\n",
    "\n",
    "            # Apply date filtering\n",
    "            filtered_df = self._apply_date_filter(\n",
    "                df, datetime_col, start_dt, end_dt, inclusive\n",
    "            )\n",
    "\n",
    "            # Update metrics\n",
    "            self.metrics['filter_operations'] += 1\n",
    "            self.metrics['rows_filtered'] += original_rows - len(filtered_df)\n",
    "\n",
    "            processing_time = time.time() - start_time\n",
    "            self.logger.info(\n",
    "                f\"Date filtering completed: {original_rows} -> {len(filtered_df)} rows \"\n",
    "                f\"(range: {start_dt.date()} to {end_dt.date()}, time: {processing_time:.3f}s)\"\n",
    "            )\n",
    "\n",
    "            return filtered_df\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Date filtering failed: {str(e)}\")\n",
    "            raise ValidationError(f\"Date filtering failed: {str(e)}\") from e\n",
    "\n",
    "    def filter_by_business_hours(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        datetime_col: str = 'datetime',\n",
    "        start_time: str = '09:30',\n",
    "        end_time: str = '16:00',\n",
    "        timezone: str = 'UTC'\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Filter data to business hours only.\"\"\"\n",
    "        try:\n",
    "            df = df.copy()\n",
    "\n",
    "            # Ensure datetime column is datetime type\n",
    "            df[datetime_col] = pd.to_datetime(df[datetime_col])\n",
    "\n",
    "            # Convert to specified timezone if needed\n",
    "            if timezone != 'UTC':\n",
    "                df[datetime_col] = df[datetime_col].dt.tz_convert(timezone)\n",
    "\n",
    "            # Extract time component\n",
    "            time_component = df[datetime_col].dt.time\n",
    "\n",
    "            # Parse business hours\n",
    "            start_time_obj = pd.to_datetime(start_time, format='%H:%M').time()\n",
    "            end_time_obj = pd.to_datetime(end_time, format='%H:%M').time()\n",
    "\n",
    "            # Apply filter\n",
    "            mask = (time_component >= start_time_obj) & (time_component <= end_time_obj)\n",
    "            filtered_df = df[mask].copy()\n",
    "\n",
    "            self.logger.info(\n",
    "                f\"Business hours filtering: {len(df)} -> {len(filtered_df)} rows \"\n",
    "                f\"(hours: {start_time}-{end_time} {timezone})\"\n",
    "            )\n",
    "\n",
    "            return filtered_df\n",
    "\n",
    "        except Exception as e:\n",
    "            raise ValidationError(f\"Business hours filtering failed: {str(e)}\") from e\n",
    "\n",
    "    def filter_by_weekdays(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        datetime_col: str = 'datetime',\n",
    "        weekdays: List[int] = None  # 0=Monday, 6=Sunday\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Filter data to specific weekdays only.\"\"\"\n",
    "        try:\n",
    "            if weekdays is None:\n",
    "                weekdays = [0, 1, 2, 3, 4]  # Monday to Friday\n",
    "\n",
    "            df = df.copy()\n",
    "            df[datetime_col] = pd.to_datetime(df[datetime_col])\n",
    "\n",
    "            # Extract weekday (0=Monday, 6=Sunday)\n",
    "            weekday_component = df[datetime_col].dt.weekday\n",
    "\n",
    "            # Apply filter\n",
    "            mask = weekday_component.isin(weekdays)\n",
    "            filtered_df = df[mask].copy()\n",
    "\n",
    "            weekday_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "            selected_days = [weekday_names[day] for day in weekdays]\n",
    "\n",
    "            self.logger.info(\n",
    "                f\"Weekday filtering: {len(df)} -> {len(filtered_df)} rows \"\n",
    "                f\"(days: {', '.join(selected_days)})\"\n",
    "            )\n",
    "\n",
    "            return filtered_df\n",
    "\n",
    "        except Exception as e:\n",
    "            raise ValidationError(f\"Weekday filtering failed: {str(e)}\") from e\n",
    "\n",
    "    def _validate_filter_inputs(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        datetime_col: str,\n",
    "        inclusive: str,\n",
    "        handle_invalid: str\n",
    "    ) -> None:\n",
    "        \"\"\"Validate filter inputs.\"\"\"\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise ValidationError(\"Input 'df' must be a pandas DataFrame\")\n",
    "\n",
    "        if datetime_col not in df.columns:\n",
    "            raise ValidationError(f\"Column '{datetime_col}' not found in DataFrame\")\n",
    "\n",
    "        if inclusive not in {'both', 'left', 'right', 'neither'}:\n",
    "            raise ValidationError(\"inclusive must be one of 'both', 'left', 'right', 'neither'\")\n",
    "\n",
    "        if handle_invalid not in {'warn', 'error', 'ignore'}:\n",
    "            raise ValidationError(\"handle_invalid must be one of 'warn', 'error', 'ignore'\")\n",
    "\n",
    "    def _parse_and_validate_dates(\n",
    "        self,\n",
    "        start_date: Union[str, datetime],\n",
    "        end_date: Union[str, datetime],\n",
    "        date_format: Optional[str]\n",
    "    ) -> tuple:\n",
    "        \"\"\"Parse and validate date inputs.\"\"\"\n",
    "        try:\n",
    "            # Convert dates to datetime if they aren't already\n",
    "            if isinstance(start_date, str):\n",
    "                start_dt = pd.to_datetime(start_date, format=date_format, errors='raise')\n",
    "            else:\n",
    "                start_dt = pd.to_datetime(start_date)\n",
    "\n",
    "            if isinstance(end_date, str):\n",
    "                end_dt = pd.to_datetime(end_date, format=date_format, errors='raise')\n",
    "            else:\n",
    "                end_dt = pd.to_datetime(end_date)\n",
    "\n",
    "        except Exception as e:\n",
    "            raise ValidationError(\n",
    "                f\"Failed to convert dates to datetime: {str(e)}. \"\n",
    "                f\"Ensure dates match the specified format.\"\n",
    "            )\n",
    "\n",
    "        # Validate date range\n",
    "        if start_dt > end_dt:\n",
    "            raise ValidationError(\"start_date must be earlier than or equal to end_date\")\n",
    "\n",
    "        return start_dt, end_dt\n",
    "\n",
    "    def _process_datetime_column(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        datetime_col: str,\n",
    "        column_date_format: Optional[str],\n",
    "        handle_invalid: str\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Process and validate datetime column.\"\"\"\n",
    "        df = df.copy()\n",
    "\n",
    "        try:\n",
    "            df[datetime_col] = pd.to_datetime(\n",
    "                df[datetime_col],\n",
    "                format=column_date_format,\n",
    "                errors='coerce'\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise ValidationError(\n",
    "                f\"Failed to convert '{datetime_col}' to datetime: {str(e)}. \"\n",
    "                f\"Ensure column data matches the specified format.\"\n",
    "            )\n",
    "\n",
    "        # Handle invalid datetime entries\n",
    "        invalid_count = df[datetime_col].isna().sum()\n",
    "        if invalid_count > 0:\n",
    "            self.metrics['invalid_dates_found'] += invalid_count\n",
    "\n",
    "            if handle_invalid == 'error':\n",
    "                raise ValidationError(\n",
    "                    f\"Found {invalid_count} invalid datetime entries in '{datetime_col}'\"\n",
    "                )\n",
    "            elif handle_invalid == 'warn':\n",
    "                self.logger.warning(\n",
    "                    f\"Found {invalid_count} invalid datetime entries in '{datetime_col}' - \"\n",
    "                    f\"these rows will be excluded\"\n",
    "                )\n",
    "            # For 'ignore', we don't log anything but still remove invalid entries\n",
    "\n",
    "            # Remove rows with invalid dates\n",
    "            df = df.dropna(subset=[datetime_col])\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _apply_date_filter(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        datetime_col: str,\n",
    "        start_dt: pd.Timestamp,\n",
    "        end_dt: pd.Timestamp,\n",
    "        inclusive: str\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Apply the actual date filtering logic.\"\"\"\n",
    "        # Filter rows based on inclusivity\n",
    "        if inclusive == 'both':\n",
    "            mask = (df[datetime_col] >= start_dt) & (df[datetime_col] <= end_dt)\n",
    "        elif inclusive == 'left':\n",
    "            mask = (df[datetime_col] >= start_dt) & (df[datetime_col] < end_dt)\n",
    "        elif inclusive == 'right':\n",
    "            mask = (df[datetime_col] > start_dt) & (df[datetime_col] <= end_dt)\n",
    "        else:  # inclusive == 'neither'\n",
    "            mask = (df[datetime_col] > start_dt) & (df[datetime_col] < end_dt)\n",
    "\n",
    "        return df[mask].copy()\n",
    "\n",
    "    def get_filter_metrics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get date filtering metrics.\"\"\"\n",
    "        return {\n",
    "            'total_filter_operations': self.metrics['filter_operations'],\n",
    "            'total_rows_filtered': self.metrics['rows_filtered'],\n",
    "            'invalid_dates_found': self.metrics['invalid_dates_found']\n",
    "        }\n",
    "\n",
    "\n",
    "# Data Validation\n",
    "class DataValidator:\n",
    "    \"\"\"Comprehensive data validation for financial time series.\"\"\"\n",
    "\n",
    "    def __init__(self, config: ResamplingConfig, logger: logging.Logger):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "        self.date_filter = EnterpriseDateFilter(logger)\n",
    "\n",
    "    def validate_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Comprehensive dataframe validation.\"\"\"\n",
    "        if df is None or df.empty:\n",
    "            raise ValidationError(\"DataFrame is None or empty\")\n",
    "\n",
    "        # Check required columns\n",
    "        self._validate_columns(df)\n",
    "\n",
    "        # Validate data types\n",
    "        df = self._validate_data_types(df)\n",
    "\n",
    "        # Validate OHLC relationships\n",
    "        if self.config.validate_ohlc:\n",
    "            self._validate_ohlc_relationships(df)\n",
    "\n",
    "        # Check for minimum data points\n",
    "        if len(df) < self.config.min_data_points:\n",
    "            raise ValidationError(f\"Insufficient data points: {len(df)} < {self.config.min_data_points}\")\n",
    "\n",
    "        # Validate datetime continuity\n",
    "        self._validate_datetime_continuity(df)\n",
    "\n",
    "        self.logger.info(f\"Successfully validated DataFrame with {len(df)} rows\")\n",
    "        return df\n",
    "\n",
    "    def _validate_columns(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"Validate required columns exist.\"\"\"\n",
    "        missing_columns = set(self.config.required_columns) - set(df.columns)\n",
    "        if missing_columns:\n",
    "            raise ValidationError(f\"Missing required columns: {missing_columns}\")\n",
    "\n",
    "    def _validate_data_types(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Validate and convert data types.\"\"\"\n",
    "        df = df.copy()\n",
    "\n",
    "        # Convert datetime column\n",
    "        try:\n",
    "            df[self.config.datetime_column_name] = pd.to_datetime(df[self.config.datetime_column_name])\n",
    "        except Exception as e:\n",
    "            raise ValidationError(f\"Failed to convert datetime column: {e}\")\n",
    "\n",
    "        # Convert numeric columns\n",
    "        for col in self.config.numeric_columns:\n",
    "            if col in df.columns:\n",
    "                try:\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "                    if df[col].isna().all():\n",
    "                        raise ValidationError(f\"Column '{col}' contains no valid numeric data\")\n",
    "                except Exception as e:\n",
    "                    raise ValidationError(f\"Failed to convert column '{col}' to numeric: {e}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _validate_ohlc_relationships(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"Validate OHLC price relationships.\"\"\"\n",
    "        ohlc_cols = ['open', 'high', 'low', 'close']\n",
    "        if not all(col in df.columns for col in ohlc_cols):\n",
    "            return\n",
    "\n",
    "        # Check high >= max(open, close) and low <= min(open, close)\n",
    "        invalid_high = df['high'] < df[['open', 'close']].max(axis=1)\n",
    "        invalid_low = df['low'] > df[['open', 'close']].min(axis=1)\n",
    "\n",
    "        if invalid_high.any():\n",
    "            count = invalid_high.sum()\n",
    "            self.logger.warning(f\"Found {count} rows where high < max(open, close)\")\n",
    "\n",
    "        if invalid_low.any():\n",
    "            count = invalid_low.sum()\n",
    "            self.logger.warning(f\"Found {count} rows where low > min(open, close)\")\n",
    "\n",
    "    def _validate_datetime_continuity(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"Check for large gaps in datetime series.\"\"\"\n",
    "        if len(df) < 2:\n",
    "            return\n",
    "\n",
    "        datetime_col = self.config.datetime_column_name\n",
    "        time_diffs = df[datetime_col].diff().dropna()\n",
    "\n",
    "        # Convert max_gap_tolerance to timedelta\n",
    "        max_gap = pd.Timedelta(self.config.max_gap_tolerance)\n",
    "\n",
    "        large_gaps = time_diffs > max_gap\n",
    "        if large_gaps.any():\n",
    "            gap_count = large_gaps.sum()\n",
    "            max_gap_size = time_diffs.max()\n",
    "            self.logger.warning(f\"Found {gap_count} large gaps in data. Maximum gap: {max_gap_size}\")\n",
    "\n",
    "\n",
    "# Caching System\n",
    "class ResamplingCache:\n",
    "    \"\"\"LRU-style cache for resampling results.\"\"\"\n",
    "\n",
    "    def __init__(self, max_size: int = 100):\n",
    "        self.max_size = max_size\n",
    "        self.cache: Dict[str, Dict] = {}\n",
    "        self.access_times: Dict[str, float] = {}\n",
    "\n",
    "    def _generate_key(self, df: pd.DataFrame, period: str, **kwargs) -> str:\n",
    "        \"\"\"Generate cache key based on data and parameters.\"\"\"\n",
    "        # Create hash from dataframe and parameters\n",
    "        df_hash = hashlib.md5(\n",
    "            pd.util.hash_pandas_object(df, index=True).values\n",
    "        ).hexdigest()\n",
    "        params_str = f\"{period}_{json.dumps(sorted(kwargs.items()))}\"\n",
    "        return f\"{df_hash}_{params_str}\"\n",
    "\n",
    "    def get(self, key: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Get cached result.\"\"\"\n",
    "        if key in self.cache:\n",
    "            self.access_times[key] = time.time()\n",
    "            return self.cache[key]['data'].copy()\n",
    "        return None\n",
    "\n",
    "    def set(self, key: str, data: pd.DataFrame) -> None:\n",
    "        \"\"\"Cache result with LRU eviction.\"\"\"\n",
    "        if len(self.cache) >= self.max_size:\n",
    "            # Remove least recently used item\n",
    "            lru_key = min(self.access_times.keys(), key=lambda k: self.access_times[k])\n",
    "            del self.cache[lru_key]\n",
    "            del self.access_times[lru_key]\n",
    "\n",
    "        self.cache[key] = {\n",
    "            'data': data.copy(),\n",
    "            'timestamp': time.time()\n",
    "        }\n",
    "        self.access_times[key] = time.time()\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear cache.\"\"\"\n",
    "        self.cache.clear()\n",
    "        self.access_times.clear()\n",
    "\n",
    "\n",
    "# Abstract Base Classes\n",
    "class ResamplingStrategy(ABC):\n",
    "    \"\"\"Abstract base class for resampling strategies.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def resample(self, df: pd.DataFrame, period: str, **kwargs) -> pd.DataFrame:\n",
    "        \"\"\"Resample the dataframe according to the strategy.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class StandardOHLCVStrategy(ResamplingStrategy):\n",
    "    \"\"\"Standard OHLCV resampling strategy.\"\"\"\n",
    "\n",
    "    def resample(self, df: pd.DataFrame, period: str, **kwargs) -> pd.DataFrame:\n",
    "        \"\"\"Standard OHLCV resampling logic.\"\"\"\n",
    "        aggregation_rules = {\n",
    "            'open': 'first',\n",
    "            'high': 'max',\n",
    "            'low': 'min',\n",
    "            'close': 'last',\n",
    "            'vol': 'sum'\n",
    "        }\n",
    "\n",
    "        # Allow custom aggregation rules\n",
    "        if 'custom_agg' in kwargs:\n",
    "            aggregation_rules.update(kwargs['custom_agg'])\n",
    "\n",
    "        # Filter only existing columns\n",
    "        available_agg = {\n",
    "            col: func for col, func in aggregation_rules.items()\n",
    "            if col in df.columns\n",
    "        }\n",
    "\n",
    "        return df.resample(period).agg(available_agg)\n",
    "\n",
    "\n",
    "# Main Enterprise Resampler Class\n",
    "class EnterpriseDataResampler:\n",
    "    \"\"\"\n",
    "    Enterprise-grade data resampling system with comprehensive features:\n",
    "    - Data validation and error handling\n",
    "    - Configurable resampling strategies\n",
    "    - Caching for performance\n",
    "    - Comprehensive logging\n",
    "    - Parallel processing support\n",
    "    - Extensible architecture\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Optional[ResamplingConfig] = None,\n",
    "        logger: Optional[logging.Logger] = None\n",
    "    ):\n",
    "        self.config = config or ResamplingConfig()\n",
    "        self.logger = logger or setup_logging()\n",
    "        self.validator = DataValidator(self.config, self.logger)\n",
    "        self.date_filter = EnterpriseDateFilter(self.logger)\n",
    "        self.cache = ResamplingCache(self.config.cache_size_limit) if self.config.enable_caching else None\n",
    "        self.strategies = {\n",
    "            'ohlcv': StandardOHLCVStrategy()\n",
    "        }\n",
    "\n",
    "        # Performance metrics\n",
    "        self.metrics = {\n",
    "            'total_operations': 0,\n",
    "            'cache_hits': 0,\n",
    "            'cache_misses': 0,\n",
    "            'errors': 0,\n",
    "            'processing_times': []\n",
    "        }\n",
    "\n",
    "        self.logger.info(\"EnterpriseDataResampler initialized successfully\")\n",
    "\n",
    "    def register_strategy(self, name: str, strategy: ResamplingStrategy) -> None:\n",
    "        \"\"\"Register a custom resampling strategy.\"\"\"\n",
    "        self.strategies[name] = strategy\n",
    "        self.logger.info(f\"Registered custom strategy: {name}\")\n",
    "\n",
    "    def resample_data(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        period: str = None,\n",
    "        strategy: str = 'ohlcv',\n",
    "        validate: bool = True,\n",
    "        **kwargs\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Main method to resample time series data.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame with time series data\n",
    "            period: Resampling period (e.g., '4H', '1D')\n",
    "            strategy: Resampling strategy to use\n",
    "            validate: Whether to validate input data\n",
    "            **kwargs: Additional arguments for resampling strategy\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Resampled data\n",
    "\n",
    "        Raises:\n",
    "            ValidationError: If input data validation fails\n",
    "            ProcessingError: If resampling process fails\n",
    "            ConfigurationError: If configuration is invalid\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        period = period or self.config.default_period\n",
    "\n",
    "        try:\n",
    "            # Validate period\n",
    "            if period not in self.config.supported_periods:\n",
    "                raise ConfigurationError(f\"Unsupported period: {period}\")\n",
    "\n",
    "            # Check cache first\n",
    "            cache_key = None\n",
    "            if self.cache:\n",
    "                cache_key = self.cache._generate_key(df, period, strategy=strategy, **kwargs)\n",
    "                cached_result = self.cache.get(cache_key)\n",
    "                if cached_result is not None:\n",
    "                    self.metrics['cache_hits'] += 1\n",
    "                    self.logger.info(f\"Cache hit for period {period}\")\n",
    "                    return cached_result\n",
    "                else:\n",
    "                    self.metrics['cache_misses'] += 1\n",
    "\n",
    "            # Validate input data\n",
    "            if validate:\n",
    "                df = self.validator.validate_dataframe(df)\n",
    "\n",
    "            # Prepare data for resampling\n",
    "            df_prepared = self._prepare_dataframe(df)\n",
    "\n",
    "            # Execute resampling strategy\n",
    "            if strategy not in self.strategies:\n",
    "                raise ConfigurationError(f\"Unknown strategy: {strategy}\")\n",
    "\n",
    "            resampled_df = self.strategies[strategy].resample(df_prepared, period, **kwargs)\n",
    "\n",
    "            # Post-process results\n",
    "            result = self._post_process_result(resampled_df)\n",
    "\n",
    "            # Cache result\n",
    "            if self.cache and cache_key:\n",
    "                self.cache.set(cache_key, result)\n",
    "\n",
    "            # Update metrics\n",
    "            processing_time = time.time() - start_time\n",
    "            self.metrics['total_operations'] += 1\n",
    "            self.metrics['processing_times'].append(processing_time)\n",
    "\n",
    "            self.logger.info(\n",
    "                f\"Successfully resampled {len(df)} rows to {len(result)} rows \"\n",
    "                f\"(period: {period}, time: {processing_time:.3f}s)\"\n",
    "            )\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            self.metrics['errors'] += 1\n",
    "            self.logger.error(f\"Resampling failed: {str(e)}\")\n",
    "            raise ProcessingError(f\"Resampling failed: {str(e)}\") from e\n",
    "\n",
    "    def batch_resample(\n",
    "        self,\n",
    "        datasets: List[Dict[str, Any]],\n",
    "        use_parallel: bool = None\n",
    "    ) -> List[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Resample multiple datasets in batch.\n",
    "\n",
    "        Args:\n",
    "            datasets: List of dicts with 'df' and resampling parameters\n",
    "            use_parallel: Whether to use parallel processing\n",
    "\n",
    "        Returns:\n",
    "            List[pd.DataFrame]: List of resampled dataframes\n",
    "        \"\"\"\n",
    "        use_parallel = use_parallel if use_parallel is not None else self.config.parallel_processing\n",
    "\n",
    "        if use_parallel and len(datasets) > 1:\n",
    "            return self._batch_resample_parallel(datasets)\n",
    "        else:\n",
    "            return self._batch_resample_sequential(datasets)\n",
    "\n",
    "    def _batch_resample_sequential(self, datasets: List[Dict[str, Any]]) -> List[pd.DataFrame]:\n",
    "        \"\"\"Sequential batch resampling.\"\"\"\n",
    "        results = []\n",
    "        for i, dataset in enumerate(datasets):\n",
    "            try:\n",
    "                df = dataset.pop('df')\n",
    "                result = self.resample_data(df, **dataset)\n",
    "                results.append(result)\n",
    "                self.logger.info(f\"Processed dataset {i+1}/{len(datasets)}\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Failed to process dataset {i+1}: {e}\")\n",
    "                results.append(None)\n",
    "        return results\n",
    "\n",
    "    def _batch_resample_parallel(self, datasets: List[Dict[str, Any]]) -> List[pd.DataFrame]:\n",
    "        \"\"\"Parallel batch resampling.\"\"\"\n",
    "        results = [None] * len(datasets)\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:\n",
    "            future_to_index = {\n",
    "                executor.submit(self.resample_data, dataset.pop('df'), **dataset): i\n",
    "                for i, dataset in enumerate(datasets)\n",
    "            }\n",
    "\n",
    "            for future in as_completed(future_to_index):\n",
    "                index = future_to_index[future]\n",
    "                try:\n",
    "                    results[index] = future.result()\n",
    "                    self.logger.info(f\"Completed parallel processing for dataset {index+1}\")\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Parallel processing failed for dataset {index+1}: {e}\")\n",
    "                    results[index] = None\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _prepare_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Prepare dataframe for resampling.\"\"\"\n",
    "        df = df.copy()\n",
    "\n",
    "        # Set datetime index\n",
    "        datetime_col = self.config.datetime_column_name\n",
    "        if datetime_col in df.columns:\n",
    "            df.set_index(datetime_col, inplace=True)\n",
    "\n",
    "        # Sort by datetime\n",
    "        df.sort_index(inplace=True)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _post_process_result(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Post-process resampling results.\"\"\"\n",
    "        # Remove empty periods\n",
    "        if self.config.drop_na_periods:\n",
    "            df.dropna(inplace=True)\n",
    "\n",
    "        # Reset index if configured\n",
    "        if self.config.reset_index:\n",
    "            df.reset_index(inplace=True)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def resample_with_date_filter(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        start_date: Union[str, datetime] = None,\n",
    "        end_date: Union[str, datetime] = None,\n",
    "        period: str = None,\n",
    "        strategy: str = 'ohlcv',\n",
    "        validate: bool = True,\n",
    "        datetime_col: str = None,\n",
    "        inclusive: str = 'both',\n",
    "        date_format: Optional[str] = None,\n",
    "        column_date_format: Optional[str] = None,\n",
    "        **kwargs\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Resample data with integrated date filtering.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame with time series data\n",
    "            start_date: Start date for filtering (optional)\n",
    "            end_date: End date for filtering (optional)\n",
    "            period: Resampling period (e.g., '4H', '1D')\n",
    "            strategy: Resampling strategy to use\n",
    "            validate: Whether to validate input data\n",
    "            datetime_col: Name of datetime column\n",
    "            inclusive: Date boundary inclusion ('both', 'left', 'right', 'neither')\n",
    "            date_format: Format for parsing date strings\n",
    "            column_date_format: Format for datetime column\n",
    "            **kwargs: Additional arguments for resampling strategy\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Filtered and resampled data\n",
    "        \"\"\"\n",
    "        datetime_col = datetime_col or self.config.datetime_column_name\n",
    "\n",
    "        try:\n",
    "            # Apply date filtering if dates are provided\n",
    "            if start_date is not None and end_date is not None:\n",
    "                df = self.date_filter.filter_by_date_range(\n",
    "                    df=df,\n",
    "                    start_date=start_date,\n",
    "                    end_date=end_date,\n",
    "                    datetime_col=datetime_col,\n",
    "                    inclusive=inclusive,\n",
    "                    date_format=date_format,\n",
    "                    column_date_format=column_date_format\n",
    "                )\n",
    "\n",
    "                if df.empty:\n",
    "                    self.logger.warning(\"Date filtering resulted in empty DataFrame\")\n",
    "                    return df\n",
    "\n",
    "            # Perform resampling on filtered data\n",
    "            result = self.resample_data(\n",
    "                df=df,\n",
    "                period=period,\n",
    "                strategy=strategy,\n",
    "                validate=validate,\n",
    "                **kwargs\n",
    "            )\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Resample with date filter failed: {str(e)}\")\n",
    "            raise ProcessingError(f\"Resample with date filter failed: {str(e)}\") from e\n",
    "\n",
    "    def filter_business_data(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        datetime_col: str = None,\n",
    "        business_hours_only: bool = True,\n",
    "        weekdays_only: bool = True,\n",
    "        start_time: str = '09:30',\n",
    "        end_time: str = '16:00',\n",
    "        timezone: str = 'UTC',\n",
    "        weekdays: List[int] = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Filter data to business hours and/or weekdays.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            datetime_col: Name of datetime column\n",
    "            business_hours_only: Filter to business hours\n",
    "            weekdays_only: Filter to weekdays only\n",
    "            start_time: Business day start time\n",
    "            end_time: Business day end time\n",
    "            timezone: Timezone for business hours\n",
    "            weekdays: List of weekdays (0=Monday, 6=Sunday)\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Filtered data\n",
    "        \"\"\"\n",
    "        datetime_col = datetime_col or self.config.datetime_column_name\n",
    "        result_df = df.copy()\n",
    "\n",
    "        try:\n",
    "            if weekdays_only:\n",
    "                result_df = self.date_filter.filter_by_weekdays(\n",
    "                    result_df, datetime_col, weekdays\n",
    "                )\n",
    "\n",
    "            if business_hours_only:\n",
    "                result_df = self.date_filter.filter_by_business_hours(\n",
    "                    result_df, datetime_col, start_time, end_time, timezone\n",
    "                )\n",
    "\n",
    "            return result_df\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Business data filtering failed: {str(e)}\")\n",
    "            raise ProcessingError(f\"Business data filtering failed: {str(e)}\") from e\n",
    "        \"\"\"Get performance metrics.\"\"\"\n",
    "        processing_times = self.metrics['processing_times']\n",
    "        avg_time = np.mean(processing_times) if processing_times else 0\n",
    "\n",
    "        return {\n",
    "            'total_operations': self.metrics['total_operations'],\n",
    "            'cache_hit_rate': (\n",
    "                self.metrics['cache_hits'] /\n",
    "                (self.metrics['cache_hits'] + self.metrics['cache_misses'])\n",
    "                if (self.metrics['cache_hits'] + self.metrics['cache_misses']) > 0 else 0\n",
    "            ),\n",
    "            'error_rate': (\n",
    "                self.metrics['errors'] / self.metrics['total_operations']\n",
    "                if self.metrics['total_operations'] > 0 else 0\n",
    "            ),\n",
    "            'average_processing_time': avg_time,\n",
    "            'cache_size': len(self.cache.cache) if self.cache else 0\n",
    "        }\n",
    "\n",
    "    def clear_cache(self) -> None:\n",
    "        \"\"\"Clear the resampling cache.\"\"\"\n",
    "        if self.cache:\n",
    "            self.cache.clear()\n",
    "            self.logger.info(\"Cache cleared successfully\")\n",
    "\n",
    "\n",
    "# Utility Functions\n",
    "def create_sample_data(\n",
    "    start_date: str = \"2023-01-01\",\n",
    "    periods: int = 1000,\n",
    "    freq: str = \"1T\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Create sample OHLCV data for testing.\"\"\"\n",
    "    date_range = pd.date_range(start=start_date, periods=periods, freq=freq)\n",
    "\n",
    "    # Generate realistic price data\n",
    "    np.random.seed(42)\n",
    "    base_price = 100\n",
    "    returns = np.random.normal(0, 0.001, periods)\n",
    "    prices = base_price * (1 + returns).cumprod()\n",
    "\n",
    "    # Generate OHLCV data\n",
    "    data = []\n",
    "    for i, (timestamp, close_price) in enumerate(zip(date_range, prices)):\n",
    "        high = close_price * (1 + abs(np.random.normal(0, 0.005)))\n",
    "        low = close_price * (1 - abs(np.random.normal(0, 0.005)))\n",
    "        open_price = prices[i-1] if i > 0 else close_price\n",
    "        volume = abs(np.random.normal(1000, 200))\n",
    "\n",
    "        data.append({\n",
    "            'datetime': timestamp,\n",
    "            'open': open_price,\n",
    "            'high': max(high, open_price, close_price),\n",
    "            'low': min(low, open_price, close_price),\n",
    "            'close': close_price,\n",
    "            'vol': volume\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd33e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create resampler instance\n",
    "resampler = EnterpriseDataResampler()\n",
    "\n",
    "# Filter by date range first\n",
    "# date_filtered = resampler.date_filter.filter_by_date_range(\n",
    "#     ohlc,\n",
    "#     start_date='2024-01-01',\n",
    "#     end_date='2024-12-31'\n",
    "# )\n",
    "\n",
    "# # Then filter to business hours\n",
    "# business_filtered = resampler.filter_business_data(\n",
    "#     date_filtered,\n",
    "#     business_hours_only=True,\n",
    "#     weekdays_only=True\n",
    "# )\n",
    "\n",
    "# Finally resample\n",
    "dta_4H_1Y = resampler.resample_data(ohlc, period='4H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3752ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dta_4H_1Y.to_parquet('DAT_ASCII_XAUUSD_H4_2024' + \".parquet\", engine=\"pyarrow\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21737e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by date range first\n",
    "date_filtered = resampler.date_filter.filter_by_date_range(\n",
    "    ohlc,\n",
    "    start_date='2024-11-01',\n",
    "    end_date='2024-12-01'\n",
    ")\n",
    "\n",
    "dta_15M_1Y = resampler.resample_data(date_filtered, period='15T')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f35627",
   "metadata": {},
   "outputs": [],
   "source": [
    "dta_15M_1Y.to_parquet('DAT_ASCII_XAUUSD_15M_2024' + \".parquet\", engine=\"pyarrow\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9eaa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by date range first\n",
    "date_filtered = resampler.date_filter.filter_by_date_range(\n",
    "    ohlc,\n",
    "    start_date='2024-11-01',\n",
    "    end_date='2024-11-07'\n",
    ")\n",
    "\n",
    "# Then filter to business hours\n",
    "business_filtered = resampler.filter_business_data(\n",
    "    date_filtered,\n",
    "    business_hours_only=False,\n",
    "    weekdays_only=True\n",
    ")\n",
    "\n",
    "print (business_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9acbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "business_filtered.to_parquet('DAT_ASCII_XAUUSD_1M_NOV1STW' + \".parquet\", engine=\"pyarrow\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Chartter-2",
   "language": "python",
   "name": "myproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
